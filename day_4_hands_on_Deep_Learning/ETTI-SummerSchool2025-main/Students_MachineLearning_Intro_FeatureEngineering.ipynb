{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabitza-tech/ETTI-SummerSchool2025/blob/main/Students_MachineLearning_Intro_FeatureEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbX2b-CGfpJo"
      },
      "source": [
        "# 🚀 PART 1 - Introduction: Binary Classification with the Adult Income Dataset\n",
        "\n",
        "In this exercise, we will explore the **Adult Income** dataset, a widely-used dataset for **classification tasks**.  \n",
        "\n",
        "The main goal of this exercise is to:  \n",
        "- Gain a solid understanding of the dataset  \n",
        "- Preprocess and prepare it for a **binary classification** task  \n",
        "- Apply Machine Learning techniques to predict income levels  \n",
        "\n",
        "---\n",
        "\n",
        "### 🛠 Tools & Libraries\n",
        "We will primarily use **Scikit-Learn**, a powerful and versatile library for **Data Science** and **Machine Learning**. Some of its highlights:  \n",
        "\n",
        "- Ready-to-use datasets for **prototyping and experimentation**  \n",
        "- Built-in **data preprocessing tools**  \n",
        "- Wide selection of **Machine Learning algorithms**  \n",
        "- Easy evaluation with common metrics such as:  \n",
        "  - ✅ Accuracy  \n",
        "  - ✅ Precision  \n",
        "  - ✅ Recall  \n",
        "  - ✅ F1-score  \n",
        "\n",
        "Scikit-Learn makes it straightforward to experiment with different models, and while it works well out-of-the-box, understanding the **hyperparameters** is important for improving performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 💾 Saving Results\n",
        "All results, including **screenshots and brief explanations**, will be saved in Google Docs for documentation purposes.\n",
        "\n",
        "---\n",
        "\n",
        "### 📥 Step 1: Load the Dataset\n",
        "Let's start by loading the **Adult Income** dataset and exploring its structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\vasid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\vasid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
            "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\vasid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\vasid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\vasid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vasid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\vasid\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.3.2-cp313-cp313-win_amd64.whl (11.0 MB)\n",
            "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.8/11.0 MB 9.9 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 5.0/11.0 MB 16.5 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 9.7/11.0 MB 18.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.0/11.0 MB 18.6 MB/s  0:00:00\n",
            "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "\n",
            "   ---------------------------------------- 0/3 [pytz]\n",
            "   ---------------------------------------- 0/3 [pytz]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   ------------- -------------------------- 1/3 [tzdata]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   -------------------------- ------------- 2/3 [pandas]\n",
            "   ---------------------------------------- 3/3 [pandas]\n",
            "\n",
            "Successfully installed pandas-2.3.2 pytz-2025.2 tzdata-2025.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "collapsed": true,
        "id": "uldeYc37hCy_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'data':        age     workclass  fnlwgt     education  education-num  \\\n",
              " 0       25       Private  226802          11th              7   \n",
              " 1       38       Private   89814       HS-grad              9   \n",
              " 2       28     Local-gov  336951    Assoc-acdm             12   \n",
              " 3       44       Private  160323  Some-college             10   \n",
              " 4       18           NaN  103497  Some-college             10   \n",
              " ...    ...           ...     ...           ...            ...   \n",
              " 48837   27       Private  257302    Assoc-acdm             12   \n",
              " 48838   40       Private  154374       HS-grad              9   \n",
              " 48839   58       Private  151910       HS-grad              9   \n",
              " 48840   22       Private  201490       HS-grad              9   \n",
              " 48841   52  Self-emp-inc  287927       HS-grad              9   \n",
              " \n",
              "            marital-status         occupation relationship   race     sex  \\\n",
              " 0           Never-married  Machine-op-inspct    Own-child  Black    Male   \n",
              " 1      Married-civ-spouse    Farming-fishing      Husband  White    Male   \n",
              " 2      Married-civ-spouse    Protective-serv      Husband  White    Male   \n",
              " 3      Married-civ-spouse  Machine-op-inspct      Husband  Black    Male   \n",
              " 4           Never-married                NaN    Own-child  White  Female   \n",
              " ...                   ...                ...          ...    ...     ...   \n",
              " 48837  Married-civ-spouse       Tech-support         Wife  White  Female   \n",
              " 48838  Married-civ-spouse  Machine-op-inspct      Husband  White    Male   \n",
              " 48839             Widowed       Adm-clerical    Unmarried  White  Female   \n",
              " 48840       Never-married       Adm-clerical    Own-child  White    Male   \n",
              " 48841  Married-civ-spouse    Exec-managerial         Wife  White  Female   \n",
              " \n",
              "        capital-gain  capital-loss  hours-per-week native-country  \n",
              " 0                 0             0              40  United-States  \n",
              " 1                 0             0              50  United-States  \n",
              " 2                 0             0              40  United-States  \n",
              " 3              7688             0              40  United-States  \n",
              " 4                 0             0              30  United-States  \n",
              " ...             ...           ...             ...            ...  \n",
              " 48837             0             0              38  United-States  \n",
              " 48838             0             0              40  United-States  \n",
              " 48839             0             0              40  United-States  \n",
              " 48840             0             0              20  United-States  \n",
              " 48841         15024             0              40  United-States  \n",
              " \n",
              " [48842 rows x 14 columns],\n",
              " 'target': 0        <=50K\n",
              " 1        <=50K\n",
              " 2         >50K\n",
              " 3         >50K\n",
              " 4        <=50K\n",
              "          ...  \n",
              " 48837    <=50K\n",
              " 48838     >50K\n",
              " 48839    <=50K\n",
              " 48840    <=50K\n",
              " 48841     >50K\n",
              " Name: class, Length: 48842, dtype: category\n",
              " Categories (2, object): ['<=50K', '>50K'],\n",
              " 'frame':        age     workclass  fnlwgt     education  education-num  \\\n",
              " 0       25       Private  226802          11th              7   \n",
              " 1       38       Private   89814       HS-grad              9   \n",
              " 2       28     Local-gov  336951    Assoc-acdm             12   \n",
              " 3       44       Private  160323  Some-college             10   \n",
              " 4       18           NaN  103497  Some-college             10   \n",
              " ...    ...           ...     ...           ...            ...   \n",
              " 48837   27       Private  257302    Assoc-acdm             12   \n",
              " 48838   40       Private  154374       HS-grad              9   \n",
              " 48839   58       Private  151910       HS-grad              9   \n",
              " 48840   22       Private  201490       HS-grad              9   \n",
              " 48841   52  Self-emp-inc  287927       HS-grad              9   \n",
              " \n",
              "            marital-status         occupation relationship   race     sex  \\\n",
              " 0           Never-married  Machine-op-inspct    Own-child  Black    Male   \n",
              " 1      Married-civ-spouse    Farming-fishing      Husband  White    Male   \n",
              " 2      Married-civ-spouse    Protective-serv      Husband  White    Male   \n",
              " 3      Married-civ-spouse  Machine-op-inspct      Husband  Black    Male   \n",
              " 4           Never-married                NaN    Own-child  White  Female   \n",
              " ...                   ...                ...          ...    ...     ...   \n",
              " 48837  Married-civ-spouse       Tech-support         Wife  White  Female   \n",
              " 48838  Married-civ-spouse  Machine-op-inspct      Husband  White    Male   \n",
              " 48839             Widowed       Adm-clerical    Unmarried  White  Female   \n",
              " 48840       Never-married       Adm-clerical    Own-child  White    Male   \n",
              " 48841  Married-civ-spouse    Exec-managerial         Wife  White  Female   \n",
              " \n",
              "        capital-gain  capital-loss  hours-per-week native-country  class  \n",
              " 0                 0             0              40  United-States  <=50K  \n",
              " 1                 0             0              50  United-States  <=50K  \n",
              " 2                 0             0              40  United-States   >50K  \n",
              " 3              7688             0              40  United-States   >50K  \n",
              " 4                 0             0              30  United-States  <=50K  \n",
              " ...             ...           ...             ...            ...    ...  \n",
              " 48837             0             0              38  United-States  <=50K  \n",
              " 48838             0             0              40  United-States   >50K  \n",
              " 48839             0             0              40  United-States  <=50K  \n",
              " 48840             0             0              20  United-States  <=50K  \n",
              " 48841         15024             0              40  United-States   >50K  \n",
              " \n",
              " [48842 rows x 15 columns],\n",
              " 'categories': None,\n",
              " 'feature_names': ['age',\n",
              "  'workclass',\n",
              "  'fnlwgt',\n",
              "  'education',\n",
              "  'education-num',\n",
              "  'marital-status',\n",
              "  'occupation',\n",
              "  'relationship',\n",
              "  'race',\n",
              "  'sex',\n",
              "  'capital-gain',\n",
              "  'capital-loss',\n",
              "  'hours-per-week',\n",
              "  'native-country'],\n",
              " 'target_names': ['class'],\n",
              " 'DESCR': '**Author**: Ronny Kohavi and Barry Becker  \\n**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Adult) - 1996  \\n**Please cite**: Ron Kohavi, \"Scaling Up the Accuracy of Naive-Bayes Classifiers: a Decision-Tree Hybrid\", Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, 1996  \\n\\nPrediction task is to determine whether a person makes over 50K a year. Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0))\\n\\nThis is the original version from the UCI repository, with training and test sets merged.\\n\\n### Variable description\\n\\nVariables are all self-explanatory except __fnlwgt__. This is a proxy for the demographic background of the people: \"People with similar demographic characteristics should have similar weights\". This similarity-statement is not transferable across the 51 different states.\\n\\nDescription from the donor of the database: \\n\\nThe weights on the CPS files are controlled to independent estimates of the civilian noninstitutional population of the US.  These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are:\\n1.  A single cell estimate of the population 16+ for each state.\\n2.  Controls for Hispanic Origin by age and sex.\\n3.  Controls by Race, age and sex.\\n\\nWe use all three sets of controls in our weighting program and \"rake\" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples, each with its own probability of selection, the statement only applies within state.\\n\\n\\n### Relevant papers  \\n\\nRonny Kohavi and Barry Becker. Data Mining and Visualization, Silicon Graphics.  \\ne-mail: ronnyk \\'@\\' live.com for questions.\\n\\nDownloaded from openml.org.',\n",
              " 'details': {'id': '1590',\n",
              "  'name': 'adult',\n",
              "  'version': '2',\n",
              "  'description_version': '15',\n",
              "  'format': 'ARFF',\n",
              "  'upload_date': '2015-06-09T16:39:06',\n",
              "  'licence': 'Public',\n",
              "  'url': 'https://api.openml.org/data/v1/download/1595261/adult.arff',\n",
              "  'parquet_url': 'https://data.openml.org/datasets/0000/1590/dataset_1590.pq',\n",
              "  'file_id': '1595261',\n",
              "  'default_target_attribute': 'class',\n",
              "  'version_label': '2',\n",
              "  'tag': ['Data Science',\n",
              "   'Economics',\n",
              "   'OpenML-CC18',\n",
              "   'OpenML100',\n",
              "   'Statistics',\n",
              "   'study_123',\n",
              "   'study_135',\n",
              "   'study_14',\n",
              "   'study_144',\n",
              "   'study_218',\n",
              "   'study_241',\n",
              "   'study_34',\n",
              "   'study_99'],\n",
              "  'visibility': 'public',\n",
              "  'original_data_url': 'https://archive.ics.uci.edu/ml/datasets/adult',\n",
              "  'minio_url': 'https://data.openml.org/datasets/0000/1590/dataset_1590.pq',\n",
              "  'status': 'active',\n",
              "  'processing_date': '2018-10-03 21:37:04',\n",
              "  'md5_checksum': 'bb6510925e5d4b23d136715febb2cdf5'},\n",
              " 'url': 'https://www.openml.org/d/1590'}"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math \n",
        "# Load Adult dataset\n",
        "adult = fetch_openml(\"adult\", version=2, as_frame=True)\n",
        "x, y = adult.data, adult.target\n",
        "\n",
        "adult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQjjq2uthE_w"
      },
      "source": [
        "# 📝 Exercise 1: Exploring the Dataset\n",
        "\n",
        "In this exercise, we will take a closer look at the dataset by examining the features (**X**) and the labels (**y**).  \n",
        "> **Hint:** The dataset is in **pandas DataFrame** format, so you can leverage all the familiar pandas methods.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. **Preview the data**: Display the first 5 samples in the dataset.\n",
        "2. **Dataset size**: How many samples are there in total?\n",
        "3. **Feature count**: How many features does each sample have?\n",
        "4. **Number of classes**: How many unique classes are present in the target variable?\n",
        "5. **Class distribution**: How many samples belong to each class?\n",
        "6. **Missing values**: Identify the total number of missing values.  \n",
        "   > **Hint:** Use the `isna()` method in pandas.\n",
        "7. **Missing value percentages**: Compute what percentage of each feature contains missing data.\n",
        "8. **Feature types**: Determine the type (categorical or numerical) of the features that contain missing values.\n",
        "\n",
        "---\n",
        "\n",
        "Take your time to explore the dataset thoroughly—this step is crucial for **data cleaning** and **preprocessing**, which can significantly impact model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xhCpSp5FhKl_",
        "outputId": "e9d954f4-2e23-4b9a-83f1-2892455c030c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "48842\n",
            "14\n",
            "48842\n",
            "['<=50K', '>50K']\n",
            "Categories (2, object): ['<=50K', '>50K']\n",
            "Samples with '>50K': 11687\n",
            "Samples with '<=50k: 37155\n",
            "age                  0\n",
            "workclass         2799\n",
            "fnlwgt               0\n",
            "education            0\n",
            "education-num        0\n",
            "marital-status       0\n",
            "occupation        2809\n",
            "relationship         0\n",
            "race                 0\n",
            "sex                  0\n",
            "capital-gain         0\n",
            "capital-loss         0\n",
            "hours-per-week       0\n",
            "native-country     857\n",
            "dtype: int64\n",
            "age                  int64\n",
            "workclass         category\n",
            "fnlwgt               int64\n",
            "education         category\n",
            "education-num        int64\n",
            "marital-status    category\n",
            "occupation        category\n",
            "relationship      category\n",
            "race              category\n",
            "sex               category\n",
            "capital-gain         int64\n",
            "capital-loss         int64\n",
            "hours-per-week       int64\n",
            "native-country    category\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Missing values in the dataset are marked with '?' => Replace \"?\" with NaN\n",
        "print(type(x))\n",
        "print(type(y))\n",
        "x = x.replace(\"?\", pd.NA)\n",
        "\n",
        "# .... Code here\n",
        "# Print first 5 samples\n",
        "a =  (x[:4])\n",
        "b =  (y[:4])\n",
        "# Get the number of samples in the dataset (rows) and the number of features (columns)\n",
        "print(x.shape[0]) \n",
        "print(x.shape[1])\n",
        "# How many classes?\n",
        "print(y.shape[0])\n",
        "# Samples per calss\n",
        "cnt = 0\n",
        "print(y.unique())\n",
        "for i in range(y.shape[0]):\n",
        "    if y.iloc[i] == '>50K':\n",
        "        cnt += 1\n",
        "print(\"Samples with '>50K':\", cnt)\n",
        "print(\"Samples with '<=50k:\",y.shape[0] - cnt)\n",
        "# How many missing values do we have? => isna().sum()\n",
        "print(x.isna().sum())\n",
        "\n",
        "# Type of features\n",
        "print(x.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chaseekblewg"
      },
      "source": [
        "# 📝 Exercise 2: Handling Missing Data\n",
        "\n",
        "Now that we have identified the missing values in the dataset, and since they represent **≤5% of the samples**, we can remove them.  \n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Drop the samples with missing values from both **features (X)** and **labels (y)**.  \n",
        "   > **Hint:** Use `dropna()`.\n",
        "2. Check the **new size** of the dataset.\n",
        "3. Calculate **how much data was lost** after removing the missing entries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6LQ2qKiFlcgg",
        "outputId": "f1fa958c-0e98-4783-c17c-ced65d8bcbaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nr sample pierdute 3620\n",
            "45222\n",
            "45222\n",
            "Procent_sample_pierdute: (8, '%')\n"
          ]
        }
      ],
      "source": [
        "# CODE HERE\n",
        "# Drop samples that have missing values for fratures and lables\n",
        "x_drop = (x.dropna())\n",
        "y_drop = y[x_drop.index]\n",
        "\n",
        "lost_sample = x.shape[0] - x_drop.shape[0]\n",
        "\n",
        "print(\"nr sample pierdute\", lost_sample)\n",
        "print(x_drop.shape[0]) \n",
        "print(y_drop.shape[0])\n",
        "# Check new dataset size\n",
        "print(\"Procent_sample_pierdute:\" ,(math.floor(lost_sample/x_drop.shape[0] * 100),\"%\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELyqM-avxJ_J"
      },
      "source": [
        "# 🔄 Transforming Categorical Data into Numbers\n",
        "\n",
        "Many Machine Learning algorithms require **numerical inputs**, so we need to convert categorical features into numbers.  \n",
        "\n",
        "For this, we will use **`LabelEncoder`** from `scikit-learn`.  \n",
        "\n",
        "> This time, we will handle this step for you. Next time, you will be on your own! 🙂\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYEyKovzxKFR",
        "outputId": "b64e350a-ca1b-494a-fab9-bd701ca2546b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['workclass', 'education', 'marital-status', 'occupation',\n",
            "       'relationship', 'race', 'sex', 'native-country'],\n",
            "      dtype='object')\n",
            "Index(['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
            "       'hours-per-week'],\n",
            "      dtype='object')\n",
            "data_type_x_encoded age               int64\n",
            "workclass         int64\n",
            "fnlwgt            int64\n",
            "education         int64\n",
            "education-num     int64\n",
            "marital-status    int64\n",
            "occupation        int64\n",
            "relationship      int64\n",
            "race              int64\n",
            "sex               int64\n",
            "capital-gain      int64\n",
            "capital-loss      int64\n",
            "hours-per-week    int64\n",
            "native-country    int64\n",
            "dtype: object\n",
            "{np.int64(0), np.int64(1)}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Numerically encode features based on the number of unique values\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Separate categorical and numeric columns\n",
        "cat_cols = x_drop.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "num_cols = x_drop.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "print(cat_cols)\n",
        "print(num_cols)\n",
        "# 1. Label Encoder - Individual numerical values for each categorical value in a feature\n",
        "x_encoded = x_drop.copy()\n",
        "for col in cat_cols:\n",
        "  x_encoded[col] = le.fit_transform(x_drop[col])\n",
        "\n",
        "# x_encoded has the same structure as x, but with categorical columns having numerical values now\n",
        "x_encoded.head()\n",
        "\n",
        "print(\"data_type_x_encoded\",x_encoded.dtypes)\n",
        "# For labels, we can simply transform them in numerical values in the case of binary classification\n",
        "y_encoded = le.fit_transform(y_drop)\n",
        "print(set(y_encoded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlRwQ7nDocOv"
      },
      "source": [
        "# 📝 Exercise 3: Train-Test Split\n",
        "\n",
        "Next, we will split our dataset into a **training set** and a **test set**.  \n",
        "\n",
        "> In real-world applications, we would usually also create a **validation set**, but for this introductory exercise, we will keep it simple with just two splits.  \n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Split the data into **train (70%)** and **test (30%)** sets using `train_test_split()`.  \n",
        "   > **Note:** Use the `stratify` parameter to ensure class proportions are preserved.\n",
        "2. Check the **number of samples** in the train and test sets.\n",
        "3. Verify that the **class distribution** is balanced in both sets.  \n",
        "   - Example: If the training set has 65% `'>=50K'` and 35% `'<=50K'`, the test set should have a **similar distribution**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hNuhgP55ocBA",
        "outputId": "fab6eb00-2106-4fd7-b8ef-4bdd424256bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 31655\n",
            "Test samples: 13567\n",
            "Test samples:        age  workclass  fnlwgt  education  education-num  marital-status  \\\n",
            "0       25          2  226802          1              7               4   \n",
            "1       38          2   89814         11              9               2   \n",
            "2       28          1  336951          7             12               2   \n",
            "3       44          2  160323         15             10               2   \n",
            "5       34          2  198693          0              6               4   \n",
            "...    ...        ...     ...        ...            ...             ...   \n",
            "48837   27          2  257302          7             12               2   \n",
            "48838   40          2  154374         11              9               2   \n",
            "48839   58          2  151910         11              9               6   \n",
            "48840   22          2  201490         11              9               4   \n",
            "48841   52          3  287927         11              9               2   \n",
            "\n",
            "       occupation  relationship  race  sex  capital-gain  capital-loss  \\\n",
            "0               6             3     2    1             0             0   \n",
            "1               4             0     4    1             0             0   \n",
            "2              10             0     4    1             0             0   \n",
            "3               6             0     2    1          7688             0   \n",
            "5               7             1     4    1             0             0   \n",
            "...           ...           ...   ...  ...           ...           ...   \n",
            "48837          12             5     4    0             0             0   \n",
            "48838           6             0     4    1             0             0   \n",
            "48839           0             4     4    0             0             0   \n",
            "48840           0             3     4    1             0             0   \n",
            "48841           3             5     4    0         15024             0   \n",
            "\n",
            "       hours-per-week  native-country  \n",
            "0                  40              38  \n",
            "1                  50              38  \n",
            "2                  40              38  \n",
            "3                  40              38  \n",
            "5                  30              38  \n",
            "...               ...             ...  \n",
            "48837              38              38  \n",
            "48838              40              38  \n",
            "48839              40              38  \n",
            "48840              20              38  \n",
            "48841              40              38  \n",
            "\n",
            "[45222 rows x 14 columns]\n",
            "Procent_sample_pierdute: (69.99911547476891, '%')\n",
            "Procent_sample_pierdute: (30.00088452523108, '%')\n",
            "Train samples: 31655\n",
            "Test samples: 13567\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Complete the function - search it\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_encoded, y_encoded, test_size=0.3,train_size=0.7, stratify=y_encoded, random_state=42)\n",
        "\n",
        "# CODE HERE - STUDENTS\n",
        "# No samples\n",
        "\n",
        "# See if train-test balanced\n",
        "print(\"Train samples:\", x_train.shape[0])\n",
        "print(\"Test samples:\", x_test.shape[0]) \n",
        "print(\"Test samples:\", x_encoded) \n",
        "print(\"Procent_sample_pierdute:\" ,((y_train.shape[0]/y_encoded.shape[0] * 100),\"%\"))\n",
        "print(\"Procent_sample_pierdute:\" ,((y_test.shape[0]/y_encoded.shape[0] * 100),\"%\"))\n",
        "\n",
        "print(\"Train samples:\", y_train.shape[0])\n",
        "print(\"Test samples:\", y_test.shape[0]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP-CsSNws2Tg"
      },
      "source": [
        "# ⚡ Training & Inference with a Simple Logistic Regression\n",
        "\n",
        "Now that our data is **cleaned**, **encoded**, and **split** into training and test sets, we can move on to **making predictions**.  \n",
        "\n",
        "In this section, we will train a **K Nearest Neighbour** classifier and a **Logistic Regression** model, two of the simplest and most widely used algorithms for **binary classification**, and then evaluate its performance on the test set.\n",
        "\n",
        "# 📝 Exercise 4: Comparing 2 Machine Learning Classification Methods\n",
        "### Tasks\n",
        "\n",
        "1. Which algorithm has the better performance? R:al doilea are accuracy mai mare\n",
        "2. Which algorithm is faster to train? Do some profiling on training time taken for each. R: al doilea are accuracy mai mare dar ii trebuie si mai mult timp, deci primul e mai rapid chiar daca nu e la fel de gresit \n",
        "3. How does changing the `n_neighbors` parameter for KNN affect results and how does `max_interations` affect the results? \n",
        "    R: devine accuracy mai bun dar \n",
        "4. Why isn't it ok to simply change the training hyper-parameters of the methods and then evaluate them on the test set?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYhfH3Fis4kW",
        "outputId": "e5d776ad-1665-4ef4-fc32-345456450ded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN training time: 0.0818777084350586\n",
            "\n",
            "KNN Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.91      0.85     10205\n",
            "           1       0.54      0.32      0.40      3362\n",
            "\n",
            "    accuracy                           0.76     13567\n",
            "   macro avg       0.67      0.61      0.63     13567\n",
            "weighted avg       0.74      0.76      0.74     13567\n",
            "\n",
            "Logistic Regression training time: 33.27241230010986\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.95      0.88     10205\n",
            "           1       0.70      0.35      0.47      3362\n",
            "\n",
            "    accuracy                           0.80     13567\n",
            "   macro avg       0.76      0.65      0.67     13567\n",
            "weighted avg       0.79      0.80      0.78     13567\n",
            "\n",
            "Accuracy: 0.8026829807621434\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\vasid\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 12562 iteration(s) (status=1):\n",
            "STOP: TOTAL NO. OF F,G EVALUATIONS EXCEEDS LIMIT\n",
            "\n",
            "You might also want to scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "t_knn = time.time()\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(x_train, y_train)\n",
        "t_knn2 = time.time() - t_knn\n",
        "print (\"KNN training time:\", t_knn2)\n",
        "# --- Make predictions ---\n",
        "y_pred = knn.predict(x_test)\n",
        "\n",
        "print(\"\\nKNN Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "t_clf = time.time()\n",
        "clf = LogisticRegression(max_iter=10000000000000)\n",
        "clf.fit(x_train, y_train)\n",
        "t_clf = time.time() - t_clf\n",
        "\n",
        "print (\"Logistic Regression training time:\", t_clf)\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(x_test)\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Alternatively, just for checking accuracy\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntLCkMWG0C9r"
      },
      "source": [
        "# 📏 Scaling Numerical Features\n",
        "\n",
        "When working with numerical data, it’s important to **scale features** so that they have a similar distribution.  \n",
        "For example, scaling values to a **range between 0 and 1** can improve the performance of many Machine Learning algorithms.\n",
        "\n",
        "Scikit-Learn provides built-in scalers, so we don’t have to implement them manually.\n",
        "\n",
        "---\n",
        "\n",
        "### Common Scaling Techniques\n",
        "\n",
        "**1. StandardScaler**  \n",
        "Scales each feature to have **mean** $\\mu = 0$ and **standard deviation** $\\sigma = 1$:  \n",
        "\n",
        "$$\n",
        "z = \\frac{x - \\mu}{\\sigma}\n",
        "$$\n",
        "\n",
        "**2. MinMaxScaler**  \n",
        "Scales each feature to a specified range, by default \\([0, 1]\\):  \n",
        "\n",
        "$$\n",
        "x' = \\frac{x - x_{\\min}}{x_{\\max} - x_{\\min}}\n",
        "$$\n",
        "\n",
        "> You can also choose other ranges, e.g., \\([-1, 1]\\).\n",
        "\n",
        "---\n",
        "\n",
        "**⚠️ Important:** Always **FIT** your scaler on the **TRAINING set** only.  \n",
        "Scaling the train and test set together can lead to **data leakage**, which will make your results **skewed, biased, and unfair**.\n",
        "\n",
        "E.g.: 'centering the values of the dataset with the mean of the entire dataset' -> can lead to influencing training with the distribution of the test set, which might be totally different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsbIAxdB_bFa"
      },
      "source": [
        "# 📝 Exercise 5: Scaling and Its Impact\n",
        "\n",
        "In this exercise, we will explore how **different scaling methods** affect model performance.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Scale your data using:  \n",
        "   - `StandardScaler()`  \n",
        "   - `MinMaxScaler()`  \n",
        "   > **Hint:** Use `fit_transform()` on the training data and `transform()` on the test data.\n",
        "2. Verify that your scaled data **looks different** from the original data.  \n",
        "   > It’s always good to double-check that scaling was applied correctly.\n",
        "3. Train **two separate Logistic Regression models**:  \n",
        "   - One on the StandardScaler data  \n",
        "   - One on the MinMaxScaler data  \n",
        "4. Compare the results. Which scaling method gives **better performance** on the test set?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Yo2nr8XN0Byb",
        "outputId": "73f3bd94-cac2-416a-ba0d-e281446eb1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.33639527 -0.21803666 -0.51625421  0.44399784  1.51020289 -1.71538391\n",
            "  -0.74030198 -0.25640362  0.38407079 -1.44321823 -0.14805735 -0.21840026\n",
            "   0.74727875  0.26588708]\n",
            " [ 1.7706406   0.82313856  0.52499458 -0.33960785  1.12030743 -1.71538391\n",
            "   1.2495518  -0.25640362  0.38407079  0.69289591 13.08181239 -0.21840026\n",
            "  -0.0844001   0.26588708]]\n",
            "---\n",
            "[[0.35616438 0.33333333 0.08282957 0.8        0.86666667 0.\n",
            "  0.23076923 0.2        1.         0.         0.         0.\n",
            "  0.5        0.95      ]\n",
            " [0.61643836 0.5        0.15753371 0.6        0.8        0.\n",
            "  0.84615385 0.2        1.         1.         1.         0.\n",
            "  0.39795918 0.95      ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# CODE HERE\n",
        "scaler_std = StandardScaler()\n",
        "X_train_std = scaler_std.fit_transform(x_train)\n",
        "X_test_std = scaler_std.transform(x_test)\n",
        "\n",
        "scaler_minmax = MinMaxScaler()\n",
        "X_train_minmax = scaler_minmax.fit_transform(x_train)\n",
        "X_test_minmax = scaler_minmax.transform(x_test)\n",
        "\n",
        "print(X_train_std[:2])\n",
        "print('---')\n",
        "print(X_train_minmax[:2])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "egwfQbql9GKG",
        "outputId": "a526d263-3f25-4cba-cb30-81855d899e99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.94      0.89     10205\n",
            "           1       0.72      0.44      0.55      3362\n",
            "\n",
            "    accuracy                           0.82     13567\n",
            "   macro avg       0.78      0.69      0.72     13567\n",
            "weighted avg       0.81      0.82      0.80     13567\n",
            "\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.94      0.89     10205\n",
            "           1       0.72      0.43      0.54      3362\n",
            "\n",
            "    accuracy                           0.82     13567\n",
            "   macro avg       0.78      0.69      0.71     13567\n",
            "weighted avg       0.81      0.82      0.80     13567\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Train Logistic Regression StandardScaler---\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_std, y_train)\n",
        "\n",
        "# --- Make predictions ---# \n",
        "y_pred = clf.predict(X_test_std)\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# --- Train Logistic Regression MinMaxScaler---\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_minmax, y_train)\n",
        "\n",
        "# --- Make predictions ---\n",
        "y_pred = clf.predict(X_test_minmax)\n",
        "print(\"\\nLogistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Knx79Ufa8B3z"
      },
      "source": [
        "# 🚀 PART 2 - Transforming Categorical Data with One-Hot Encoding\n",
        "\n",
        "Many Machine Learning algorithms require **numerical input**, so categorical features must be converted into numbers.  \n",
        "\n",
        "This time, we will use **`OneHotEncoder`** from `scikit-learn`, which creates a **binary column for each category** in a feature.  \n",
        "\n",
        "> Good news: You already have the One-Hot Encoded data ready as `X_encoded` and `y_encoded`. (lucky you!)  \n",
        "\n",
        "However, you will need to **repeat the previous steps** with this new encoding:  \n",
        "- Split the data into train and test sets  \n",
        "- Scale the numerical features if necessary  \n",
        "- Train the model  \n",
        "- Compare the results with the previous **LabelEncoder** approach  \n",
        "\n",
        "This will help you understand **how encoding choices impact model performance**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i80sQXHw8CBq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# One-Hot Encode Features\n",
        "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
        "\n",
        "# Separate categorical and numeric columns\n",
        "cat_cols = x_drop.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "num_cols = x_drop.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# One-Hot Encode categorical features - Each feature, will be One-Hot Encoded\n",
        "x_encoded_array = ohe.fit_transform(x_drop[cat_cols])\n",
        "\n",
        "# Convert encoded array back to DataFrame\n",
        "x_encoded_df = pd.DataFrame(\n",
        "    x_encoded_array,\n",
        "    columns=ohe.get_feature_names_out(cat_cols),\n",
        "    index=x_drop.index\n",
        ")\n",
        "\n",
        "# Combine numeric columns and encoded categorical columns\n",
        "x_encoded = pd.concat([x_drop[num_cols], x_encoded_df], axis=1)\n",
        "\n",
        "# x_encoded now has more columns than the original x, but with categorical columns one-hot encoded\n",
        "x_encoded.head()\n",
        "\n",
        "# For labels, we can simply transform them in numerical values in the case of binary classification\n",
        "y_encoded = le.fit_transform(y_drop)\n",
        "print(set(y_encoded))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfc6SmPs872S"
      },
      "source": [
        "# 📝 Exercise 6: One-Hot Encoding and Its Impact\n",
        "\n",
        "In this exercise, we will evaluate how **One-Hot Encoding** affects model performance compared to Label Encoding.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Determine **how many features** the One-Hot Encoded dataset now contains.\n",
        "2. Split the data into **train and test sets**, keeping the **same proportions** as in the first case for a fair comparison.\n",
        "3. Train **two Logistic Regression classifiers**:  \n",
        "   - One with scaled features  \n",
        "   - One without scaling\n",
        "4. Compare the results and analyze:  \n",
        "   - What are the **performance gains or losses** with One-Hot Encoding compared to Label Encoding?\n",
        "5. Is training faster or slower than previously? Do some profiling of time taken for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fOKT9JL9YmD"
      },
      "outputs": [],
      "source": [
        "# Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oa0I32GBPCD"
      },
      "source": [
        "# 📝 Exercise 7 (Optional - Homework): Handling Missing Values\n",
        "\n",
        "For the curious minds: so far, we simply **dropped samples** with missing values.  \n",
        "But what if we **keep them** and try to fill in the missing information?  \n",
        "\n",
        "### Tasks\n",
        "\n",
        "1. Treat the missing values (`'n/a'`) as a **separate category** for categorical features.  \n",
        "   > Hint: This is very easy to do in pandas.\n",
        "2. Fill the missing values:  \n",
        "   - **Categorical features:** use the **most frequent value** in the column  \n",
        "   - **Numerical features:** use the **mean or median** of the column\n",
        "3. Train your model again and **compare the results**.  \n",
        "   - Does filling missing values improve performance, or not?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JIWUojtB8xr"
      },
      "outputs": [],
      "source": [
        "# Code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1miRc-LXk3a"
      },
      "source": [
        "# 📝 Exercise 8 (Optional - Homework): Exploring Interesting Relationships\n",
        "\n",
        "At the end of the day, the goal of data analysis and predictions is to **generate insights** that can have a real-world impact.  \n",
        "This exercise encourages you to explore **interesting patterns and relationships** in the Adult Income dataset.\n",
        "\n",
        "### Suggested Questions to Explore\n",
        "\n",
        "1. Where do people earning **>=50K/year** come from?  \n",
        "2. How many hours do they work in each occupation or category?  \n",
        "3. What is the **average age** in each category?  \n",
        "4. How is the **gender balance** for each income category?  \n",
        "5. How many hours do people in each category work?  \n",
        "6. Any **other observations** that you find interesting or surprising.  \n",
        "\n",
        "> Feel free to use **groupby**, **pivot tables**, or **visualizations** to uncover meaningful patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "175_18WaX297"
      },
      "outputs": [],
      "source": [
        "# Code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMmw4yAWnpxBRKB9kufOCLT",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
